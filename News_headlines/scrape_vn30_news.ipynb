{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "# import os\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [\"thị%20trường%20chứng%20khoán\", \"vn30\", \"thị%20trường%20tài%20chính\", \"vn-index\"]\n",
    "news_time_list = []\n",
    "news_headline_list = []\n",
    "news_subtitle_list = []\n",
    "combined_news_list = []\n",
    "for key_word in key_words:\n",
    "    year = 2024\n",
    "    for i in list(range(1,100)):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\") # for Chrome >= 109\n",
    "\n",
    "        # Set the download directory preference\n",
    "        # download_directory = os.path.abspath(\"C:/Users/Admin/Downloads/News data scraping\")\n",
    "        # prefs = {\"download.default_directory\": download_directory}\n",
    "        # chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        website = f\"https://cafebiz.vn/timelinesearch/{key_word}/{i}.htm\"\n",
    "        # path = \"C/Users/Admin/Documents/Old D disk/Web scraping/\"\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(website)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        time.sleep(2)\n",
    "        all_news = driver.find_elements(By.XPATH, '//div[@class=\"infodsthome\"]')\n",
    "        \n",
    "        if not all_news:\n",
    "            driver.quit()\n",
    "            # If no news found, break out of the loop\n",
    "            break\n",
    "        for news in all_news:\n",
    "            published_time = news.find_element(By.XPATH, \"./p[@class='cate']\").text\n",
    "            # Convert the time string to a datetime object\n",
    "            published_time = datetime.strptime(published_time, \"%d/%m/%Y - %H:%M\")\n",
    "            year = int(published_time.year)\n",
    "            if year <= 2023:\n",
    "                break\n",
    "\n",
    "            news_headline = news.find_element(By.XPATH, './h3').text\n",
    "            news_subtitle = news.find_element(By.XPATH, \"./p[@class='sapo']\").text\n",
    "\n",
    "            # Combine headline and subtitle and add to the new list\n",
    "            combined_news_list.append(f\"{news_headline}: {news_subtitle}\")\n",
    "\n",
    "            # Add the unique news entry to your lists\n",
    "            news_time_list.append(published_time)\n",
    "            news_headline_list.append(news_headline)\n",
    "            news_subtitle_list.append(news_subtitle)\n",
    "\n",
    "        if year <= 2023:\n",
    "            driver.quit()\n",
    "            break\n",
    "\n",
    "        print(news_time_list[-1])\n",
    "        print(year)\n",
    "\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Published_time': news_time_list, 'news_content': combined_news_list, 'news_headline': news_headline_list})\n",
    "df.to_csv('cafebiz/News_headlines_cafebiz_data_update.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 02:02:48\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dateutil import parser\n",
    "\n",
    "key_words = [\"thị%20trường%20chứng%20khoán\", \"thị%20trường%20tài%20chính\"]\n",
    "news_time_list = []\n",
    "news_headline_list = []\n",
    "news_subtitle_list = []\n",
    "combined_news_list = []\n",
    "\n",
    "for key_word in key_words:\n",
    "    year = 2024\n",
    "    for i in list(range(1,100)):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\") # for Chrome >= 109\n",
    "\n",
    "        # Set the download directory preference\n",
    "        # download_directory = os.path.abspath(\"C:/Users/Admin/Downloads/News data scraping\")\n",
    "        # prefs = {\"download.default_directory\": download_directory}\n",
    "        # chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        website = f\"https://search.hemera.com.vn/search/1/{key_word}/time/{i}\"\n",
    "        # path = \"C/Users/Admin/Documents/Old D disk/Web scraping/\"\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(website)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Extract title, PublishedDate, and Sapo\n",
    "        all_news = driver.find_element(By.XPATH, '//body').text\n",
    "        if not all_news:\n",
    "            driver.quit()\n",
    "            # If no news found, break out of the loop\n",
    "            break\n",
    "\n",
    "        # Load JSON data\n",
    "        all_news = json.loads(all_news)\n",
    "\n",
    "        for item in all_news[\"List\"]:\n",
    "            # Extracting information\n",
    "            published_time = item[\"PublishedDate\"]\n",
    "            published_time = parser.parse(published_time)\n",
    "            published_time = published_time.replace(microsecond=0).replace(tzinfo=None)\n",
    "\n",
    "            year = int(published_time.year)\n",
    "            if year <= 2023:\n",
    "                break\n",
    "            news_headline = item[\"Title\"]\n",
    "            news_subtitle = item[\"Sapo\"]\n",
    "\n",
    "            # Combine headline and subtitle and add to the new list\n",
    "            combined_news_list.append(f\"{news_headline}: {news_subtitle}\")\n",
    "            # Add the unique news entry to your lists\n",
    "            news_time_list.append(published_time)\n",
    "            news_headline_list.append(news_headline)\n",
    "            news_subtitle_list.append(news_subtitle)\n",
    "\n",
    "        if year <= 2023:\n",
    "            driver.quit()\n",
    "            break\n",
    "        print(news_time_list[-1])\n",
    "        print(year)\n",
    "\n",
    "        driver.quit()\n",
    "df = pd.DataFrame({'Published_time': news_time_list, 'news_content': combined_news_list, 'news_headline': news_headline_list})\n",
    "df.to_csv('vneconomy/News_headlines_vneconomy_data_update.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:58\n",
      "2024\n",
      "2021-01-20 10:24\n",
      "2021\n",
      "2021-01-19 20:00\n",
      "2021\n",
      "2021-01-19 10:00\n",
      "2021\n",
      "2021-01-18 19:30\n",
      "2021\n",
      "2021-01-18 11:04\n",
      "2021\n",
      "2021-01-17 19:57\n",
      "2021\n",
      "2021-01-16 10:30\n",
      "2021\n",
      "2021-01-15 10:58\n",
      "2021\n",
      "2021-01-14 19:30\n",
      "2021\n",
      "2021-01-14 08:45\n",
      "2021\n",
      "2021-01-13 15:35\n",
      "2021\n",
      "2021-01-13 06:00\n",
      "2021\n",
      "2021-01-12 13:00\n",
      "2021\n",
      "2021-01-11 20:00\n",
      "2021\n",
      "2021-01-11 13:05\n",
      "2021\n",
      "2021-01-11 06:00\n",
      "2021\n",
      "2021-01-09 10:54\n",
      "2021\n",
      "2021-01-08 13:00\n",
      "2021\n",
      "2021-01-07 19:30\n",
      "2021\n",
      "2021-01-07 09:33\n",
      "2021\n",
      "2021-01-06 16:20\n",
      "2021\n",
      "2021-01-06 08:39\n",
      "2021\n",
      "2021-01-05 15:40\n",
      "2021\n",
      "2021-01-05 09:00\n",
      "2021\n",
      "2021-01-04 17:12\n",
      "2021\n",
      "2021-01-04 13:09\n",
      "2021\n",
      "2021-01-04 08:51\n",
      "2021\n",
      "2021-01-03 09:00\n",
      "2021\n",
      "2021-01-01 12:00\n",
      "2021\n",
      "2020-12-31 10:00\n",
      "2020\n",
      "2020-12-30 17:22\n",
      "2020\n",
      "2020-12-30 10:34\n",
      "2020\n",
      "2020-12-29 17:10\n",
      "2020\n",
      "2020-12-29 11:10\n",
      "2020\n",
      "2020-12-28 20:00\n",
      "2020\n",
      "2020-12-28 13:58\n",
      "2020\n",
      "2020-12-28 08:38\n",
      "2020\n",
      "2020-12-26 19:45\n",
      "2020\n",
      "2020-12-25 13:45\n",
      "2020\n",
      "2020-12-24 19:30\n",
      "2020\n",
      "2020-12-24 11:15\n",
      "2020\n",
      "2020-12-23 20:00\n",
      "2020\n",
      "2020-12-23 11:14\n",
      "2020\n",
      "2020-12-22 17:06\n",
      "2020\n",
      "2020-12-22 11:07\n",
      "2020\n",
      "2020-12-21 20:00\n",
      "2020\n",
      "2020-12-21 11:06\n",
      "2020\n",
      "2020-12-20 13:30\n",
      "2020\n",
      "2020-12-18 17:02\n",
      "2020\n",
      "2020-12-18 10:04\n",
      "2020\n",
      "2020-12-17 17:10\n",
      "2020\n",
      "2020-12-17 09:39\n",
      "2020\n",
      "2020-12-16 15:25\n",
      "2020\n",
      "2020-12-16 11:03\n",
      "2020\n",
      "2020-12-15 16:09\n",
      "2020\n",
      "2020-12-15 09:15\n",
      "2020\n",
      "2020-12-14 17:07\n",
      "2020\n",
      "2020-12-14 09:06\n",
      "2020\n",
      "2020-12-12 20:30\n",
      "2020\n",
      "2020-12-11 17:05\n",
      "2020\n",
      "2020-12-11 08:00\n",
      "2020\n",
      "2020-12-10 15:12\n",
      "2020\n",
      "2020-12-10 08:00\n",
      "2020\n",
      "2020-12-09 13:00\n",
      "2020\n",
      "2020-12-08 17:30\n",
      "2020\n",
      "2020-12-08 12:02\n",
      "2020\n",
      "2020-12-07 20:00\n",
      "2020\n",
      "2020-12-07 13:13\n",
      "2020\n",
      "2020-12-06 19:00\n",
      "2020\n",
      "2020-12-05 10:00\n",
      "2020\n",
      "2020-12-04 11:36\n",
      "2020\n",
      "2020-12-03 16:23\n",
      "2020\n",
      "2020-12-03 10:00\n",
      "2020\n",
      "2020-12-02 16:54\n",
      "2020\n",
      "2020-12-02 10:29\n",
      "2020\n",
      "2020-12-01 16:00\n",
      "2020\n",
      "2020-12-01 10:00\n",
      "2020\n",
      "2020-11-30 15:25\n",
      "2020\n",
      "2020-11-30 09:30\n",
      "2020\n",
      "2020-11-28 11:03\n",
      "2020\n",
      "2020-11-27 12:36\n",
      "2020\n",
      "2020-11-26 18:27\n",
      "2020\n",
      "2020-11-26 11:25\n",
      "2020\n",
      "2020-11-25 16:00\n",
      "2020\n",
      "2020-11-24 20:00\n",
      "2020\n",
      "2020-11-24 11:00\n",
      "2020\n",
      "2020-11-23 20:00\n",
      "2020\n",
      "2020-11-23 11:26\n",
      "2020\n",
      "2020-11-22 18:00\n",
      "2020\n",
      "2020-11-20 17:06\n",
      "2020\n",
      "2020-11-19 18:21\n",
      "2020\n",
      "2020-11-19 10:00\n",
      "2020\n",
      "2020-11-18 16:47\n",
      "2020\n",
      "2020-11-18 08:00\n",
      "2020\n",
      "2020-11-17 10:53\n",
      "2020\n",
      "2020-11-16 17:00\n",
      "2020\n",
      "2020-11-16 10:00\n",
      "2020\n",
      "2020-11-14 20:30\n",
      "2020\n",
      "2020-11-13 15:30\n",
      "2020\n",
      "2020-11-12 17:01\n",
      "2020\n",
      "2020-11-12 10:04\n",
      "2020\n",
      "2020-11-11 17:05\n",
      "2020\n",
      "2020-11-11 09:06\n",
      "2020\n",
      "2020-11-10 15:55\n",
      "2020\n",
      "2020-11-09 20:30\n",
      "2020\n",
      "2020-11-09 13:00\n",
      "2020\n",
      "2020-11-09 08:46\n",
      "2020\n",
      "2020-11-08 07:30\n",
      "2020\n",
      "2020-11-06 13:50\n",
      "2020\n",
      "2020-11-05 17:02\n",
      "2020\n",
      "2020-11-05 10:16\n",
      "2020\n",
      "2020-11-04 17:08\n",
      "2020\n",
      "2020-11-04 06:00\n",
      "2020\n",
      "2020-11-03 13:44\n",
      "2020\n",
      "2020-11-02 20:17\n",
      "2020\n",
      "2020-11-02 12:01\n",
      "2020\n",
      "2020-11-01 13:30\n",
      "2020\n",
      "2020-10-30 17:14\n",
      "2020\n",
      "2020-10-29 19:30\n",
      "2020\n",
      "2020-10-29 09:21\n",
      "2020\n",
      "2020-10-28 12:04\n",
      "2020\n",
      "2020-10-27 16:29\n",
      "2020\n",
      "2020-10-27 09:00\n",
      "2020\n",
      "2020-10-26 15:30\n",
      "2020\n",
      "2020-10-26 06:00\n",
      "2020\n",
      "2020-10-24 10:42\n",
      "2020\n",
      "2020-10-23 08:33\n",
      "2020\n",
      "2020-10-22 15:42\n",
      "2020\n",
      "2020-10-21 21:20\n",
      "2020\n",
      "2020-10-21 08:00\n",
      "2020\n",
      "2020-10-20 09:32\n",
      "2020\n",
      "2020-10-19 13:00\n",
      "2020\n",
      "2020-10-18 08:30\n",
      "2020\n",
      "2020-10-16 12:02\n",
      "2020\n",
      "2020-10-15 16:58\n",
      "2020\n",
      "2020-10-15 08:49\n",
      "2020\n",
      "2020-10-14 15:25\n",
      "2020\n",
      "2020-10-13 19:30\n",
      "2020\n",
      "2020-10-13 08:00\n",
      "2020\n",
      "2020-10-12 13:00\n",
      "2020\n",
      "2020-10-11 08:30\n",
      "2020\n",
      "2020-10-09 16:48\n",
      "2020\n",
      "2020-10-09 12:01\n",
      "2020\n",
      "2020-10-08 18:38\n",
      "2020\n",
      "2020-10-08 11:00\n",
      "2020\n",
      "2020-10-07 17:05\n",
      "2020\n",
      "2020-10-07 10:23\n",
      "2020\n",
      "2020-10-06 18:10\n",
      "2020\n",
      "2020-10-06 12:05\n",
      "2020\n",
      "2020-10-05 20:45\n",
      "2020\n",
      "2020-10-05 11:34\n",
      "2020\n",
      "2020-10-04 07:30\n",
      "2020\n",
      "2020-10-02 13:15\n",
      "2020\n",
      "2020-10-01 19:30\n",
      "2020\n",
      "2020-10-01 10:00\n",
      "2020\n",
      "2020-09-30 16:00\n",
      "2020\n",
      "2020-09-30 08:00\n",
      "2020\n",
      "2020-09-29 13:39\n",
      "2020\n",
      "2020-09-28 19:30\n",
      "2020\n",
      "2020-09-28 10:00\n",
      "2020\n",
      "2020-09-26 10:31\n",
      "2020\n",
      "2020-09-25 08:00\n",
      "2020\n",
      "2020-09-24 11:49\n",
      "2020\n",
      "2020-09-23 18:18\n",
      "2020\n",
      "2020-09-23 09:47\n",
      "2020\n",
      "2020-09-22 16:43\n",
      "2020\n",
      "2020-09-21 17:00\n",
      "2020\n",
      "2020-09-21 08:00\n",
      "2020\n",
      "2020-09-18 15:41\n",
      "2020\n",
      "2020-09-17 15:45\n",
      "2020\n",
      "2020-09-16 18:09\n",
      "2020\n",
      "2020-09-16 08:00\n",
      "2020\n",
      "2020-09-15 10:41\n",
      "2020\n",
      "2020-09-14 16:19\n",
      "2020\n",
      "2020-09-14 09:52\n",
      "2020\n",
      "2020-09-12 20:00\n",
      "2020\n",
      "2020-09-11 10:25\n",
      "2020\n",
      "2020-09-10 13:00\n",
      "2020\n",
      "2020-09-09 17:16\n",
      "2020\n",
      "2020-09-09 10:41\n",
      "2020\n",
      "2020-09-08 15:38\n",
      "2020\n",
      "2020-09-08 09:00\n",
      "2020\n",
      "2020-09-07 14:00\n",
      "2020\n",
      "2020-09-06 13:00\n",
      "2020\n",
      "2020-09-04 14:00\n",
      "2020\n",
      "2020-09-03 18:21\n",
      "2020\n",
      "2020-09-03 09:30\n",
      "2020\n",
      "2020-09-01 16:19\n",
      "2020\n",
      "2020-09-01 08:44\n",
      "2020\n",
      "2020-08-31 14:00\n",
      "2020\n",
      "2020-08-30 15:00\n",
      "2020\n",
      "2020-08-28 14:38\n",
      "2020\n",
      "2020-08-27 18:11\n",
      "2020\n",
      "2020-08-27 09:06\n",
      "2020\n",
      "2020-08-26 13:00\n",
      "2020\n",
      "2020-08-25 15:30\n",
      "2020\n",
      "2020-08-25 09:00\n",
      "2020\n",
      "2020-08-24 15:13\n",
      "2020\n",
      "2020-08-24 08:26\n",
      "2020\n",
      "2020-08-22 14:00\n",
      "2020\n",
      "2020-08-21 10:02\n",
      "2020\n",
      "2020-08-20 15:29\n",
      "2020\n",
      "2020-08-19 20:00\n",
      "2020\n",
      "2020-08-19 06:00\n",
      "2020\n",
      "2020-08-18 13:32\n",
      "2020\n",
      "2020-08-17 19:30\n",
      "2020\n",
      "2020-08-17 11:00\n",
      "2020\n",
      "2020-08-15 19:45\n",
      "2020\n",
      "2020-08-14 09:00\n",
      "2020\n",
      "2020-08-13 14:33\n",
      "2020\n",
      "2020-08-13 05:44\n",
      "2020\n",
      "2020-08-12 13:00\n",
      "2020\n",
      "2020-08-11 18:30\n",
      "2020\n",
      "2020-08-11 08:00\n",
      "2020\n",
      "2020-08-10 11:40\n",
      "2020\n",
      "2020-08-08 20:00\n",
      "2020\n",
      "2020-08-07 09:56\n",
      "2020\n",
      "2020-08-06 15:40\n",
      "2020\n",
      "2020-08-06 10:00\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "news_time_list = []\n",
    "news_headline_list = []\n",
    "news_subtitle_list = []\n",
    "combined_news_list = []\n",
    "year = 2024\n",
    "\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\") # for Chrome >= 109\n",
    "\n",
    "website = f\"https://vietstock.vn/chung-khoan.htm\"\n",
    "# path = \"C/Users/Admin/Documents/Old D disk/Web scraping/\"\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(website)\n",
    "driver.maximize_window()\n",
    "daterange = driver.find_element(By.XPATH, '//input[@name=\"daterange\"]')\n",
    "daterange.clear()\n",
    "daterange.send_keys(\"2020-01-01 - 2021-01-21\")\n",
    "\n",
    "while year > 2019:\n",
    "\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, '//div[@id=\"channel-container\"]')))\n",
    "    all_news = driver.find_elements(By.XPATH, '//div[@id=\"channel-container\"]')\n",
    "\n",
    "    if not all_news:\n",
    "        driver.quit()\n",
    "        # If no news found, break out of the loop\n",
    "        break\n",
    "    for news in all_news:\n",
    "        news = news.find_element(By.XPATH, './/div[@class=\"single_post_text\"]')\n",
    "        news_headline = news.find_element(By.XPATH, './/h4').text\n",
    "        temp_element = news.find_element(By.XPATH, \".//a[2]\")\n",
    "\n",
    "        # Extract the href attribute and text content from the element\n",
    "        href_attribute = temp_element.get_attribute('href')\n",
    "        published_day_month = temp_element.text\n",
    "        \n",
    "        match = re.search(r'/(\\d{4})/', href_attribute)\n",
    "        published_year = int(match.group(1)) if match else None\n",
    "\n",
    "        # Combine the extracted year with the text content\n",
    "        combined_datetime_str = f\"{published_year}/{published_day_month}\"\n",
    "        try:\n",
    "            # Convert the combined string to a datetime object\n",
    "            published_time = datetime.strptime(combined_datetime_str, \"%Y/%d/%m %H:%M\")\n",
    "        except:\n",
    "            published_time = datetime.now()\n",
    "\n",
    "        year = int(published_time.year)\n",
    "        if year <= 2019:\n",
    "            break\n",
    "\n",
    "        news_subtitle = news.find_element(By.XPATH, \".//p\").text\n",
    "\n",
    "        # Combine headline and subtitle and add to the new list\n",
    "        combined_news_list.append(f\"{news_headline}: {news_subtitle}\")\n",
    "\n",
    "        # Add the unique news entry to your lists\n",
    "        news_time_list.append(published_time.strftime(\"%Y-%m-%d %H:%M:%s\"))\n",
    "        news_headline_list.append(news_headline)\n",
    "        news_subtitle_list.append(news_subtitle)\n",
    "\n",
    "    if year <= 2019:\n",
    "        driver.quit()\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \".//li[@id='page-next ']/a\")))\n",
    "        next_page = driver.find_element(By.XPATH, \".//li[@id='page-next ']/a\")\n",
    "        next_page.click()\n",
    "    except:\n",
    "        time.sleep(8)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \".//li[@id='page-next ']/a\")))\n",
    "        next_page = driver.find_element(By.XPATH, \".//li[@id='page-next ']/a\")\n",
    "        next_page.click()\n",
    "    print(news_time_list[-1])\n",
    "    print(year)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Published_time': news_time_list, 'news_content': combined_news_list, 'news_headline': news_headline_list})\n",
    "df.to_csv('vietstock/News_headlines_vietstock_data_4.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\") # for Chrome >= 109\n",
    "\n",
    "# Set the download directory preference\n",
    "# download_directory = os.path.abspath(\"C:/Users/Admin/Downloads/News data scraping\")\n",
    "# prefs = {\"download.default_directory\": download_directory}\n",
    "# chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "website = \"https://www.forbes.com/advisor/investing/fed-funds-rate-history/\"\n",
    "# path = \"C/Users/Admin/Documents/Old D disk/Web scraping/\"\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(website)\n",
    "driver.maximize_window()\n",
    "\n",
    "table = driver.find_element(By.XPATH, \"//table[@id='footable_336565']\")\n",
    "rows = table.find_elements(By.XPATH, \"./tbody/tr\")\n",
    "FOMC_date = []\n",
    "interest_rate_change = []\n",
    "federal_funds_rate = []\n",
    "\n",
    "for row in rows:\n",
    "    date = row.find_element(By.XPATH, './td[1]').text\n",
    "    rate_change = row.find_element(By.XPATH, './td[2]').text\n",
    "    fed_rate = row.find_element(By.XPATH, './td[3]').text\n",
    "\n",
    "    FOMC_date.append(date)\n",
    "    interest_rate_change.append(rate_change)\n",
    "    federal_funds_rate.append(fed_rate)\n",
    "\n",
    "df = pd.DataFrame({'FOMC_date': FOMC_date, 'interest_rate_change': interest_rate_change, 'federal_funds_rate': federal_funds_rate})\n",
    "df.to_csv(\"Fed Rate Hikes 2022-2023 data.csv\", index=False, encoding='utf-8-sig')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
